\section*{Motivation \& Related Work}

There has been substantial anecdotal evidence suggesting that AI-assisted coding tools such as ChatGPT \cite{openai_chatgpt} and GitHub Copilot have significantly increased developer productivity. However, recent empirical research has shown mixed results\cite{cui2025the, paradis2024doesaiimpactdevelopment, peng2023impactaideveloperproductivity}. While some studies find notable efficiency gains from integrating generative AI into software development workflows—for instance, faster code completion and reduced debugging time—other work points to limited or highly context-dependent effects, with improvements varying by task complexity, developer experience, and integration depth.

Despite this growing literature, much of the existing evidence is correlational or based on controlled experiments detached from real-world settings. We still know little about how sudden changes in access to such tools affect developers in practice. In particular, the short-lived ChatGPT ban in Italy in early 2023 \cite{garante_ban_2023} offers a rare quasi-experimental opportunity to observe developers’ behavioral response to an abrupt disruption in access to a key AI tool.

Building on Kreitmeir and Raschky \cite{Kreitmeir2023}, who documented a 50\% decline of release events in Italian open source projects during the ban, this research moves one step closer to identifying how this productivity loss materialized. However, the effect on the granular, day-to-day development process remains unquantified. It is unclear if developers wrote less code, or if the quality of their code suffered, leading to downstream effects. This is a critical question, as several studies now link AI assistance to negative quality outcomes, such an increased likelihood of introducing security vulnerabilities \cite{fu2025securityweaknessescopilotgeneratedcode, pearce2021asleepkeyboardassessingsecurity}. This research fills that gap by investigating how the sudden removal of a key AI tool impacts the fundamental activities of coding: the volume and quality of contributions.


\section*{Methodology \& Data}

The analysis will use historical GitHub activity data from users in Italy, Austria, and France for the weeks surrounding the late March/early April 2023 ban. To this end, we will leverage the GH Archive dataset, which provides a comprehensive record of public GitHub events, including \textit{PushEvents}, \textit{ReleaseEvents}, and \textit{IssueEvents} \cite{github_archive}. Because GH Archive data does not include information about developers’ locations, we will infer user countries based on their public profile information, following the methods from prior research \cite{Kreitmeir2023}. This information will be retrieved through the GitHub GraphQL API \cite{github_graphql_api}.

We will employ a \textbf{Difference-in-Differences (DiD)} quasi-experimental design to ensure causal inference, replicating the framework from the original paper. The analysis will compare the change in developer metrics in Italy (treatment group) against those in Austria and France (control group). We will collect and analyze data from before, during, and after then ban period.

In contrast to the original study—which measured aggregate productivity through project releases—our analysis zooms in on code-level and issue-level metrics that capture the day-to-day development process. The main dependent variables are:

\begin{itemize}
  \item \textbf{Code Volume}: The number of lines of code (LOC) added and deleted per developer per day. This provides a granular measure of active coding behavior.
  \item \textbf{Bug-Related Issue Creation}: As an indirect quality proxy, we analyze whether the number of bug-related issues (identified through issue titles and labels containing keywords such as “bug,” “fix,” or “error”) increased in the weeks following the ban within affected repositories.
\end{itemize}

All variables will be standardized at the developer-day level to account for individual differences in coding activity. Control variables will include developer experience (tenure on GitHub), project size, and temporal trends (day-of-week effects).


\section*{Research Hypotheses}

\begin{itemize}
  \item \textbf{H1 (Volume)}: The ChatGPT ban caused a statistically significant reduction in the daily volume of code contributed by developers in the treatment group, measured through lines of code added and deleted per user per day.
  \item \textbf{H2 (Defect Incidence)}: Projects affected by the ban experienced an increase in bug-related issue creation in the subsequent weeks compared to the control group, indicating that the temporary loss of ChatGPT led to a higher defect rate or more post-release debugging effort.
\end{itemize}


\section*{Project Outline}



\begin{enumerate}
  \item \textbf{Data Collection \& Preparation}: First, we will gather GitHub event data from GH Archive, which we join with user data from the GitHub GraphQL API to infer developer locations. We will collect data of developers from Italy, Austria, and France for the weeks surrounding the ban period in early April 2023. As event data does not contain code changes or issue details, we will use the GitHub REST API \cite{github_api} to fetch corresponding details, such as LOC added/deleted and issue details. Finally, we will integrate data from all three sources to construct a unified dataset containing relevant activity metrics at the developer-day level.
  \item \textbf{Replication \& Validation}: Then, we reproduce the main result from Kreitmeir and Raschky — the temporary 50\% drop in Release events for Italian developers.
  \item \textbf{Extension \& Analysis}: Finally, we apply the validated model to our new dependent variables (Volume and Defect Incidence) to test hypotheses H1 and H2. Together, these analyses will offer a richer understanding of how generative AI tools influence developer productivity — not only in terms of total output but also in terms of the quality of the produced code.
\end{enumerate}
